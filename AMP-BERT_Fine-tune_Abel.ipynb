{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1omec12AxxXj4lrjYNCDivKMzWsGJcDnZ","timestamp":1682614959603},{"file_id":"174Qh22KCga8E4EiJ9fc8AUAbsQzOP11b","timestamp":1682016459129}],"private_outputs":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cYhGE2zhpcgo"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","source":["!pip install --upgrade accelerate"],"metadata":{"id":"PggjmYnSk0Sx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","import re\n","\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from torch.utils.data import Dataset, DataLoader\n","\n","torch.cuda.is_available()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","from transformers import AutoTokenizer, Trainer, TrainingArguments, BertForSequenceClassification, AdamW"],"metadata":{"id":"c8wIZMbgrdGa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(torch.version.cuda)"],"metadata":{"id":"SF4XqMAEJxC7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define a class for the AMP data that will correctly format the sequence information\n","# for fine-tuning with huggingface API\n","# the input dataframe columns must be formatted the same way as the given example\n","\n","class amp_data():\n","    def __init__(self, df, tokenizer_name='Rostlab/prot_bert_bfd', max_len=200):\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n","        self.max_len = max_len\n","\n","        self.seqs, self.labels = self.get_seqs_labels()\n","\n","    def get_seqs_labels(self):\n","        # isolate the amino acid sequences and their respective AMP labels\n","        seqs = list(df['aa_seq'])\n","        labels = list(df['AMP'].astype(int))\n","\n","#         assert len(seqs) == len(labels)\n","        return seqs, labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n","        seq_ids = self.tokenizer(seq, truncation=True, padding='max_length', max_length=self.max_len)\n","\n","        sample = {key: torch.tensor(val) for key, val in seq_ids.items()}\n","        sample['labels'] = torch.tensor(self.labels[idx])\n","\n","        return sample"],"metadata":{"id":"lb579FMwrdKU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read in the train dataset\n","# create an amp_data class of the dataset\n","\n","data_url = '/content/archivo_concatenado_Bert.csv'\n","df = pd.read_csv(data_url, index_col = 0)\n","df = df.sample(frac=1, random_state = 0)\n","print(df.head(7))\n","\n","train_dataset = amp_data(df)"],"metadata":{"id":"BBymtbXgrdOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df\n"],"metadata":{"id":"bIlEC6Ix9r5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Cargar el DataFrame desde la URL\n","data_url = '/content/archivo_concatenado_Bert.csv'\n","df = pd.read_csv(data_url, index_col=0)\n","\n","# Mezclar los datos aleatoriamente utilizando el método frac=1 (todos los datos) y random_state para reproducibilidad\n","df = df.sample(frac=1, random_state=0)\n","\n","# Dividir el DataFrame en conjuntos de entrenamiento y evaluación (80% para entrenamiento y 20% para evaluación)\n","train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Luego, puedes utilizar la función amp_data para preparar los datos para el entrenamiento con el modelo de lenguaje\n","train_dataset = amp_data(train_df)\n","eval_dataset = amp_data(eval_df)"],"metadata":{"id":"ITxp3oTs5ECd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the necessary metrics for performance evaluation\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n","    acc = accuracy_score(labels, preds)\n","#     conf = confusion_matrix(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","#         'confusion matrix': conf\n","    }"],"metadata":{"id":"n4jrGQT6rdRA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the model initializing function for Trainer in huggingface\n","\n","def model_init():\n","    return BertForSequenceClassification.from_pretrained('Rostlab/prot_bert_bfd')"],"metadata":{"id":"j-pQako9rdTQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Default title text\n","# training on entire data\n","# no evaluation/validation\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    #num_train_epochs=15,\n","    num_train_epochs=10,\n","    learning_rate = 2e-5,\n","    per_device_train_batch_size=1,\n","    warmup_steps=0,\n","    weight_decay=0.1,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    do_train=True,\n","    do_eval=True,\n","    evaluation_strategy=\"no\",\n","    save_strategy='no',\n","    gradient_accumulation_steps=64,\n","    fp16=True,\n","    fp16_opt_level=\"O2\",\n","    run_name=\"AMP-BERT\",\n","    seed=0,\n","    load_best_model_at_end = True\n",")"],"metadata":{"id":"hk0ETN8brdVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model_init=model_init,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    compute_metrics = compute_metrics,\n",")"],"metadata":{"id":"HzyjbkPi4oPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"3MEK-Cej4sgx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_results = trainer.evaluate(eval_dataset=eval_dataset)\n","\n","# Obtener el valor de la pérdida (loss) del modelo en el conjunto de evaluación\n","eval_loss = eval_results['eval_loss']\n","print(f\"Loss en el conjunto de evaluación: {eval_loss:.2f}\")"],"metadata":{"id":"SAdAeias42Px"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# performance metrics on the training data itself\n","\n","predictions, label_ids, metrics = trainer.predict(train_dataset)\n","metrics"],"metadata":{"id":"NVOmwxYarr4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the model, if desired\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","trainer.save_model('/content/drive/MyDrive/Colab Notebooks/AMP-BERT/Fine-tuned_model_Gaby_dados/')"],"metadata":{"id":"f6JgOpbw9q4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predict AMP/non-AMP for a single example\n","\n","# IMPORTANT:\n","# one must mount their Google Drive and load their own fine-tuned model before running the below cell for individual predictions\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# load appropriate tokenizer and fine-tuned model\n","tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False)\n","model = BertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/AMP-BERT/Fine-tuned_model\")"],"metadata":{"id":"xqdIvYee4i_v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["--------"],"metadata":{"id":"MqezWrC8-WWa"}},{"cell_type":"code","source":["# predict AMP/non-AMP for a single example (default ex. is from external test data: DRAMP00126)\n","\n","#@markdown **Input peptide sequence (upper case only)**\n","input_seq = 'YTNGGGGSGSSDRLSITIRPRY' #@param {type:\"string\"}\n","input_seq_spaced = ' '.join([ input_seq[i:i+1] for i in range(0, len(input_seq), 1) ])\n","input_seq_spaced = re.sub(r'[UZOB]', 'X', input_seq_spaced)\n","input_seq_tok = tokenizer(input_seq_spaced, return_tensors = 'pt')\n","\n","output = model(**input_seq_tok)\n","logits = output[0]\n","\n","# extract AMP class probability and make binary prediction\n","y_prob = torch.sigmoid(logits)[:,1].detach().numpy()\n","y_pred = y_prob > 0.5\n","if y_pred == True:\n","  input_class = 'AMP'\n","else:\n","  input_class = 'non-AMP'\n","print(y_pred)\n","print(y_prob)\n","print('Input peptide sequence: ' + input_seq)\n","print('Class prediction: ' + input_class)"],"metadata":{"id":"3q3GujE3_csL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["------------"],"metadata":{"id":"awfCfwkNSPZ8"}},{"cell_type":"markdown","source":["MASK"],"metadata":{"id":"gsq-R25Ty7xq"}},{"cell_type":"code","source":["#!apt install git-lfs"],"metadata":{"id":"TZgTQtX6zC9W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read in the train dataset\n","# create an amp_data class of the dataset\n","\n","data_url2 = '/content/AMP_pos.csv'\n","df = pd.read_csv(data_url2, index_col = 0)\n","df = df.sample(frac=1, random_state = 0)\n","print(df.head(7))\n","\n","train_dataset2 = amp_data(df)"],"metadata":{"id":"C2P4Y06Xzr8q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForMaskedLM\n","model2 = AutoModelForMaskedLM.from_pretrained('Rostlab/prot_bert_bfd')"],"metadata":{"id":"3PEEOkmT17Pw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer"],"metadata":{"id":"pjTI_NzwwfV8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False)"],"metadata":{"id":"7TVfpaFl3ZX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForLanguageModeling\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"],"metadata":{"id":"mRTLQgOu2zeV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_checkpoint = 'Rostlab/prot_bert_bfd'"],"metadata":{"id":"7yxNUxil4Cld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=5,\n","    learning_rate = 5e-5,\n","    per_device_train_batch_size=1,\n","    warmup_steps=0,\n","    weight_decay=0.1,\n","    logging_dir='./logs',\n","    logging_steps=100,\n","    do_train=True,\n","    do_eval=True,\n","    evaluation_strategy=\"no\",\n","    save_strategy='no',\n","    gradient_accumulation_steps=64,\n","    fp16=True,\n","    fp16_opt_level=\"O2\",\n","    run_name=\"AMP-BERT_MASK\",\n","    seed=0,\n","    load_best_model_at_end = True\n",")"],"metadata":{"id":"9o9B7fIq1tXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer2 = Trainer(\n","    model=model2,\n","    args=training_args,\n","    train_dataset=train_dataset2,\n","    #eval_dataset=eval_dataset2,\n","    data_collator=data_collator,\n","    compute_metrics = compute_metrics,\n",")\n"],"metadata":{"id":"d5ObCeTN2ywM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer2.train()"],"metadata":{"id":"iyFt0GWV4_J4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"pf6wuomOOVmS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save the model, if desired\n","\n","trainer2.save_model('/content/drive/MyDrive/Colab Notebooks/AMP-BERT/Fine-tuned_MASK3_model_Gaby_dataset/')"],"metadata":{"id":"omMqGo_BHKYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForMaskedLM\n","\n","model_mask = BertForMaskedLM.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/AMP-BERT/Fine-tuned_MASK_model\")"],"metadata":{"id":"K1jBTZUTHd1w"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CRJAYQfROFa"},"source":["from transformers import BertForMaskedLM, BertTokenizer, pipeline\n","tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False )\n","#model_mask = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert_bfd\")\n","unmasker = pipeline('fill-mask', model=model_mask, tokenizer=tokenizer)\n","unmasker('Y S C [MASK] F A D S L S K')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yj5mPwDM-b5x"},"source":["from transformers import BertForMaskedLM, BertTokenizer, pipeline\n","#tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False )\n","model_mask2 = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert_bfd\")\n","unmasker = pipeline('fill-mask', model=model_mask2, tokenizer=tokenizer)\n","unmasker('Y T N G G G G S G S G D R L S I T I R P [MASK] Y')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----"],"metadata":{"id":"8kBrAB6WcEl2"}},{"cell_type":"markdown","source":["Predict multiple peptides"],"metadata":{"id":"L-VR5ZMvSOmC"}},{"cell_type":"code","source":["# Open the file containing the FASTA-formatted list of words\n","with open(\"/content/new_desing9.fasta\", \"r\") as file:\n","\n","    # Read in the contents of the file as a string\n","    file_contents = file.read()\n","\n","    # Split the contents into separate sequences\n","    sequences = file_contents.split(\">\")[1:]\n","\n","    # Iterate over each sequence and extract the sequence ID and nucleotide sequence\n","    for seq in sequences:\n","        seq_id, *seq_lines = seq.split(\"\\n\")\n","        seq = \"\".join(seq_lines)\n","\n","        seq_spaced = ' '.join([ seq[i:i+1] for i in range(0, len(seq), 1) ])\n","        seq_spaced = re.sub(r'[UZOB]', 'X', seq_spaced)\n","        seq_tok = tokenizer(seq_spaced, return_tensors = 'pt')\n","\n","        output = model(**seq_tok)\n","        logits = output[0]\n","\n","        # extract AMP class probability and make binary prediction\n","        y_prob = torch.sigmoid(logits)[:,1].detach().numpy()\n","        y_pred = y_prob > 0.5\n","        if y_pred == True:\n","           input_class = 'AMP'\n","        else:\n","           input_class = 'non-AMP'\n","        #print(y_pred)\n","        print(y_prob)\n","        #print('Input peptide sequence: ' + seq)\n","        #print('Class prediction: ' + input_class)\n","\n"],"metadata":{"id":"y2y0CmuYpyY0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-------------------"],"metadata":{"id":"WM5xxK2oASL8"}},{"cell_type":"code","source":["!pip install lime"],"metadata":{"id":"NxKuqYb77QmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_seq_spaced = 'G L F S T V K G I L K'"],"metadata":{"id":"MVzG48Ky6Zyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import numpy as np\n","import lime\n","import torch\n","import torch.nn.functional as F\n","from lime.lime_text import LimeTextExplainer\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","#filename_model = 'Rostlab/prot_bert_bfd'\n","#tokenizer = AutoTokenizer.from_pretrained(filename_model)\n","#model = BertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/AMP-BERT/Fine-tuned_model\")\n","class_names = ['positive','negative', 'neutral']\n","\n","model = model.cuda()\n","\n","def predictor(input_seq_spaced):\n","    outputs = model(**tokenizer(input_seq_spaced, return_tensors=\"pt\", padding=True))\n","    tensor_logits = outputs[0]\n","    probas = F.softmax(tensor_logits).detach().numpy()\n","    return probas\n","\n","#text = 'Building more bypasses will help the environment by reducing pollution and traffic jams in towns and cities.'\n","print(tokenizer(input_seq_spaced, return_tensors='pt', padding=True))\n","\n","explainer = LimeTextExplainer(class_names=class_names)\n","exp = explainer.explain_instance(input_seq_spaced, predictor, num_features=20, num_samples=2000)\n","exp.show_in_notebook(text=input_seq_spaced)"],"metadata":{"id":"JnSQ18uy5zJb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["--------------"],"metadata":{"id":"jn9yRwUNkJgx"}},{"cell_type":"markdown","metadata":{"id":"-mon7pVlvQta"},"source":["### Applying transformers interpret"]},{"cell_type":"markdown","source":["---------------------"],"metadata":{"id":"1BOx186-pRFy"}},{"cell_type":"markdown","source":["----------------"],"metadata":{"id":"BEs1MCKduJog"}},{"cell_type":"markdown","source":["**SHAP**"],"metadata":{"id":"anCkOkyIj62K"}},{"cell_type":"code","source":["#!pip install --quiet shap==0.39\n","!pip install shap\n","!pip install xformers\n","\n","import shap\n","import transformers\n","\n","from transformers import (AutoTokenizer,\n","                          AutoModelForSequenceClassification,\n","                          TextClassificationPipeline)"],"metadata":{"id":"HqBbXcNlNgbj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n"],"metadata":{"id":"-A7n4N_oirzf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def score_and_visualize(text):\n","  prediction = pipe([text])\n","  print(prediction[0])\n","\n","  explainer = shap.Explainer(pipe)\n","  shap_values = explainer([text])\n","\n","  shap.plots.text(shap_values)\n"],"metadata":{"id":"zx4AKkFijGcy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score_and_visualize(input_seq_spaced)"],"metadata":{"id":"tcFWpG99jLmZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----"],"metadata":{"id":"uZuWk8xjzy6F"}},{"cell_type":"code","source":["model_gpu = model.cuda()"],"metadata":{"id":"C0t4H1q7yV0G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = [x[0] for x in sorted(model.config.label2id.items(), key=lambda x: x[1])]\n","\n","def model_prediction_gpu(x):\n","    tv = torch.tensor([tokenizer.encode(v, padding='max_length',\n","                                        max_length=512, truncation=True) for v in x]).cuda()\n","    attention_mask = (tv!=0).type(torch.int64).cuda()\n","    outputs = model_gpu(tv, attention_mask=attention_mask)[0]\n","    scores = torch.nn.Softmax(dim=-1)(outputs)\n","    val = torch.logit(scores).detach().cpu().numpy()\n","\n","    return val\n","\n","gpu_explainer = shap.Explainer(model_prediction_gpu, tokenizer, output_names=labels)\n","\n","shap_values = gpu_explainer(\n","    [input_seq_spaced]\n",")\n","\n","output = shap.plots.text(shap_values)"],"metadata":{"id":"HG-0YiebzzlS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["--------"],"metadata":{"id":"xSc494ju7pm3"}},{"cell_type":"code","source":["!pip install transformers_interpret"],"metadata":{"id":"2mt4GJ6LGiiE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers_interpret import MultiLabelClassificationExplainer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel"],"metadata":{"id":"zGSupC7nGZaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5Pac5jTynir"},"outputs":[],"source":["# With both the model and tokenizer initialized we are now able to get explanations on an example text.\n","cls_explainer = MultiLabelClassificationExplainer(model, tokenizer)\n","\n","word_attributions = cls_explainer(input_seq_spaced)\n","\n","# show output\n","word_attributions\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"5SfCQZVpTFQC"},"execution_count":null,"outputs":[]}]}